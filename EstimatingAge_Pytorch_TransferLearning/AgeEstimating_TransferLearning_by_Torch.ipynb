{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AgeEstimating-TransferLearning-by-Torch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "Mw6RlcTgNSsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle"
      ],
      "metadata": {
        "id": "rtH3a_bpuEvJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d jangedoo/utkface-new\n",
        "!unzip -qq utkface-new.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LFKSe_nuetw",
        "outputId": "dfbc473c-54f8-4ef2-d287-88c1d9039923"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading utkface-new.zip to /content\n",
            " 98% 324M/331M [00:03<00:00, 70.0MB/s]\n",
            "100% 331M/331M [00:03<00:00, 94.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1B7dkHP1_tr2"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import torch, torchvision, cv2, os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"AgeEstimating-TransferLearning-by-Torch\")"
      ],
      "metadata": {
        "id": "B-BFwJt91m-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = torchvision.models.resnet50(pretrained=True)"
      ],
      "metadata": {
        "id": "wrOZFJ7c_07L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning"
      ],
      "metadata": {
        "id": "L6SGhcHW1zzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, 1)"
      ],
      "metadata": {
        "id": "gTHzVyNHENy7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This freezes layers 1-6 in the total 10 layers of Resnet50"
      ],
      "metadata": {
        "id": "f8mpAqxR2amd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ct = 0\n",
        "for child in model.children():\n",
        "    ct += 1\n",
        "    if ct < 7:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "7fQDDHiw2YRz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "U5j90sRKEoLB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = wandb.config\n",
        "config.learning_rate = 0.01\n",
        "config.batch_size = 32\n",
        "config.epochs = 15"
      ],
      "metadata": {
        "id": "pGUWeTMTNe67"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "fwYICDFuurZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "ages = []\n",
        "\n",
        "for image_name in os.listdir('crop_part1')[0:9000]:\n",
        "    part = image_name.split('_')\n",
        "    ages.append(int(part[0]))\n",
        "\n",
        "    image = cv2.imread(f'crop_part1/{image_name}')\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    images.append(image)"
      ],
      "metadata": {
        "id": "tOgdutWXuptB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = pd.Series(images, name= 'Images')\n",
        "ages = pd.Series(ages, name= 'Ages')\n",
        "\n",
        "df = pd.concat([images, ages], axis= 1)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nQTKWVjBuxNy",
        "outputId": "0d6df14f-1bd7-46e0-de32-46738b82485d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fb63a82d-d849-4ec8-8b72-c3c5a29e5ff2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Images</th>\n",
              "      <th>Ages</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[[253, 236, 216], [253, 237, 214], [255, 237,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[[47, 28, 13], [49, 30, 15], [48, 30, 16], [4...</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[[245, 245, 245], [246, 246, 246], [247, 245,...</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[[51, 45, 49], [45, 39, 43], [35, 30, 34], [2...</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[[167, 135, 114], [166, 134, 113], [169, 137,...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb63a82d-d849-4ec8-8b72-c3c5a29e5ff2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fb63a82d-d849-4ec8-8b72-c3c5a29e5ff2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fb63a82d-d849-4ec8-8b72-c3c5a29e5ff2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              Images  Ages\n",
              "0  [[[253, 236, 216], [253, 237, 214], [255, 237,...     1\n",
              "1  [[[47, 28, 13], [49, 30, 15], [48, 30, 16], [4...    68\n",
              "2  [[[245, 245, 245], [246, 246, 246], [247, 245,...    52\n",
              "3  [[[51, 45, 49], [45, 39, 43], [35, 30, 34], [2...    27\n",
              "4  [[[167, 135, 114], [166, 134, 113], [169, 137,...     4"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "under4 = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if df['Ages'].iloc[i] <= 4:\n",
        "        under4.append(df.iloc[i])\n",
        "\n",
        "under4 = pd.DataFrame(under4)\n",
        "under4 = under4.sample(frac= 0.3)\n",
        "\n",
        "up4 = df[df['Ages'] > 4]\n",
        "\n",
        "df = pd.concat([under4, up4])"
      ],
      "metadata": {
        "id": "WaY4Ddg_uyLE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['Ages'] < 90]\n",
        "plt.figure(figsize=(24, 8))\n",
        "plt.hist(df['Ages'], bins= 89)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "mMaG-eCWu4S5",
        "outputId": "362d674d-80f2-44db-8ef1-e8b4e827ee6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "width = height = 224\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    df['Images'].iloc[i] = cv2.resize(df['Images'].iloc[i], (width, height))\n",
        "\n",
        "    X.append(df['Images'].iloc[i])\n",
        "    Y.append(df['Ages'].iloc[i])\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnh98PKau4IN",
        "outputId": "7ac9b5df-fc97-409d-84f4-e474f288d386"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2)"
      ],
      "metadata": {
        "id": "_P-UTvXKu3-s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y, transform=None):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "cHNd1qSyvYBx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transform = transforms.Compose([\n",
        "                                     transforms.ToPILImage(),\n",
        "                                     transforms.RandomHorizontalFlip(),\n",
        "                                     transforms.Resize((70, 70)),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "                                     ])\n",
        "\n",
        "\n",
        "dataset = MyDataset(X_train, Y_train, data_transform)\n",
        "train_data_loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "2sI8Sm46MlFL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile"
      ],
      "metadata": {
        "id": "YswSCCNe2rbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "loss_function = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "Y-_UxagGUDXf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Ej2qTt5K3BXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()"
      ],
      "metadata": {
        "id": "qcVH-VKnxIUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.watch(model)\n",
        "\n",
        "for epoch in range(config.epochs):\n",
        "    train_loss = 0.0\n",
        "    for images, labels in train_data_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # 1- forwarding\n",
        "        preds = model(images)\n",
        "        # 2- backwarding \n",
        "        loss = loss_function(preds, labels.float())\n",
        "        loss.backward()\n",
        "        # 3- Update\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss\n",
        "    \n",
        "    total_loss = train_loss / len(train_data_loader)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        wandb.log({\"loss\": total_loss})\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Loss: {total_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wWWiqo5x76c",
        "outputId": "6550fb34-c689-43a2-e255-e76ada1a7311"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 554.1524047851562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 544.5789184570312\n",
            "Epoch: 2, Loss: 541.218017578125\n",
            "Epoch: 3, Loss: 541.533203125\n",
            "Epoch: 4, Loss: 539.135009765625\n",
            "Epoch: 5, Loss: 536.3722534179688\n",
            "Epoch: 6, Loss: 538.1422119140625\n",
            "Epoch: 7, Loss: 536.1314086914062\n",
            "Epoch: 8, Loss: 536.3677368164062\n",
            "Epoch: 9, Loss: 539.5089721679688\n",
            "Epoch: 10, Loss: 534.5001220703125\n",
            "Epoch: 11, Loss: 536.3848876953125\n",
            "Epoch: 12, Loss: 534.1768798828125\n",
            "Epoch: 13, Loss: 535.8567504882812\n",
            "Epoch: 14, Loss: 536.9237060546875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"age-estimating-resnet50-torch.pth\")"
      ],
      "metadata": {
        "id": "OBAUgfssycIN"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}